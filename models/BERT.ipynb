{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT MODEL\n",
    "This python file mainly contains code for training, testing and evaluation of Pre-trained model based AKE models for BERT.\n",
    "  1. complete the parameter setting: weight = 'bert-base-uncased'\n",
    "  2. load the data and construct the dataset. Include words, cognitive features and labels.\n",
    "  3. Build the model and start training, adding cognitive features.\n",
    "  4. conduct testing, read the optimal model parameters and data, build the model, then make predictions and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 'bert-base-uncased'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "max_len = 35"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_path = '../datas/Election/train.json'\n",
    "# train_path = '../datas/daily life/train.json'\n",
    "test_path = '../datas/Election/test.json'\n",
    "# test_path = '../datas/daily life/test.json'\n",
    "\n",
    "train_file = json.load(open(train_path,'r',encoding='utf-8'))\n",
    "test_file = json.load(open(test_path, 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all words, eye-tracking signals, EEG signals and tags from training json to list\n",
    "train_sens, train_tags = [],[]\n",
    "train_Feature = []\n",
    "train_word_nums = []\n",
    "\n",
    "sens = ''\n",
    "nums = 0\n",
    "for key in train_file.keys():\n",
    "    tags = []\n",
    "    features = []\n",
    "    items = train_file[key]\n",
    "    sens = ''\n",
    "    nums = 0\n",
    "    for item in items:\n",
    "        sens += item[0]\n",
    "        sens += ' '\n",
    "        features.append(item[1:-1])               # ET+EEG: [1: -1]\n",
    "        tags.append(item[-1])\n",
    "        nums += 1\n",
    "    train_sens.append(sens.strip())\n",
    "    train_word_nums.append(nums)\n",
    "    train_Feature.append(features)\n",
    "    train_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all words, eye-tracking signals, EEG signals and tags from testing json to list\n",
    "test_sens, test_tags = [],[]\n",
    "test_Feature = []\n",
    "test_word_nums = []\n",
    "\n",
    "sens = ''\n",
    "nums = 0\n",
    "for key in test_file.keys():\n",
    "    tags = []\n",
    "    features = []\n",
    "    items = test_file[key]\n",
    "    sens = ''\n",
    "    nums = 0\n",
    "    for item in items:\n",
    "        sens += item[0]\n",
    "        sens += ' '\n",
    "        features.append(item[1:-1])                # ET+EEG: [1: -1]\n",
    "        tags.append(item[-1])\n",
    "        nums += 1\n",
    "    test_sens.append(sens.strip())\n",
    "    test_word_nums.append(nums)\n",
    "    test_Feature.append(features)\n",
    "    test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3027"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ids = {'none': 0, 'B': 1, 'I': 2, 'E': 3, 'S': 4, \"O\": 5}\n",
    "# label_to_ids = {'O': 0, 'B': 1, 'I': 2, 'E': 3, 'S': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def align_label(text,labels,features):\n",
    "  input = tokenizer(text, max_length=max_len, add_special_tokens=True, padding='max_length', truncation=True, return_tensors='pt')\n",
    "  word_ids = input.word_ids()\n",
    "  input_ids = input['input_ids'] \n",
    "  tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "  \n",
    "  previous_word_idx = None\n",
    "  new_labels = []\n",
    "  new_features = []\n",
    "  no_features = [0 for i in range(1,26)]\n",
    "\n",
    "  for word_idx in word_ids:\n",
    "      if word_idx is None:\n",
    "          new_labels.append('none')\n",
    "          new_features.append(no_features)\n",
    "        #   new_labels.append('O')\n",
    "\n",
    "      elif word_idx != previous_word_idx:\n",
    "          try:\n",
    "              new_labels.append(labels[word_idx])\n",
    "              new_features.append(features[word_idx])\n",
    "          except:\n",
    "              new_labels.append('none')\n",
    "              new_features.append(no_features)\n",
    "            #   new_labels.append('O')\n",
    "      else:\n",
    "          try:\n",
    "              new_labels.append(labels[word_idx] if label_all_tokens else 'none')\n",
    "              new_features.append(features[word_idx] if label_all_tokens else no_features)\n",
    "            #   new_labels.append(labels[word_idx] if label_all_tokens else 'O')\n",
    "          except:\n",
    "              new_labels.append('none')\n",
    "              new_features.append(no_features)\n",
    "      previous_word_idx = word_idx\n",
    "\n",
    "  label_ids = [label_to_ids[label] for label in new_labels]\n",
    "\n",
    "  return label_ids, tokens, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, old_features, tags):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.old_features = old_features\n",
    "        \n",
    "        self.labels = []\n",
    "        self.tokens = []\n",
    "        self.features = []\n",
    "        \n",
    "        self.input_ids = None\n",
    "        self.attention_masks = None\n",
    "\n",
    "    def encode(self):\n",
    "        for i in tqdm(range(len(self.texts))):\n",
    "          text = self.texts[i]\n",
    "          tag = self.tags[i]\n",
    "          feature = self.old_features[i]\n",
    "          tags, tokens, features = align_label(text, tag, feature)\n",
    "          self.labels.append(tags)\n",
    "          self.tokens.append(tokens)\n",
    "          self.features.append(features)\n",
    "          \n",
    "        self.features = np.array(self.features,float)\n",
    "        self.inputs = tokenizer(self.texts, max_length=max_len, add_special_tokens=True, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        self.input_ids = self.inputs['input_ids']\n",
    "        self.attention_masks = self.inputs['attention_mask']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx,:], self.attention_masks[idx,:], self.tokens[idx], torch.tensor(self.features[idx],dtype=torch.float32), torch.tensor(self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24210/24210 [00:07<00:00, 3182.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_sens, train_Feature, train_tags)\n",
    "train_dataset.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3027/3027 [00:01<00:00, 2964.39it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset(test_sens, test_Feature, test_tags)\n",
    "test_dataset.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BertNerModel(nn.Module):\n",
    "    def __init__(self,num_labels):\n",
    "        super(BertNerModel,self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(weight)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768+25,num_labels)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,extra_features,token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        pooled_output = outputs[0]\n",
    "        bert_outputs = self.dropout(pooled_output)\n",
    "        \n",
    "        outputs = torch.concat((bert_outputs,extra_features[:,:,:]),-1)\n",
    "        # outputs = bert_outputs\n",
    "        outputs = self.classifier(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TagConvert(raw_tags, words_set, poss=None):\n",
    "    true_tags = []\n",
    "    for i in range(raw_tags.shape[0]):\n",
    "      kw_list = []\n",
    "      nkw_list = \"\"\n",
    "      for j in range(len(raw_tags[i])):\n",
    "          item = raw_tags[i][j]\n",
    "          if item == 0:\n",
    "              continue\n",
    "          if poss !=None and j in poss[i]:\n",
    "              continue\n",
    "          # if item == 5:\n",
    "          #     continue\n",
    "          if item == 4:\n",
    "              kw_list.append(str(words_set[j][i]))\n",
    "          if item == 1:\n",
    "              nkw_list += str(words_set[j][i])\n",
    "          if item == 2:\n",
    "              nkw_list += \" \"\n",
    "              nkw_list += str(words_set[j][i])\n",
    "          if item == 3:\n",
    "              nkw_list += \" \"\n",
    "              nkw_list += str(words_set[j][i])\n",
    "              kw_list.append(nkw_list)\n",
    "              nkw_list = \"\"\n",
    "\n",
    "      true_tags.append(kw_list)\n",
    "    return true_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_data, target_data, topk=3):\n",
    "  TRUE_COUNT, PRED_COUNT, GOLD_COUNT = 0.0, 0.0, 0.0\n",
    "  for index, words in enumerate(predict_data):\n",
    "      y_pred, y_true = None, target_data[index]\n",
    "\n",
    "      if type(predict_data) == str:\n",
    "          words = sorted(words.items(), key=lambda item: (-item[1], item[0]))\n",
    "          y_pred = [i[0] for i in words]\n",
    "      elif type(predict_data) == list:\n",
    "          y_pred = words\n",
    "\n",
    "      y_pred = y_pred[0: topk]\n",
    "      TRUE_NUM = len(set(y_pred) & set(y_true))\n",
    "      TRUE_COUNT += TRUE_NUM\n",
    "      PRED_COUNT += len(y_pred)\n",
    "      GOLD_COUNT += len(y_true)\n",
    "  # compute P\n",
    "  if PRED_COUNT != 0:\n",
    "      p = (TRUE_COUNT / PRED_COUNT)\n",
    "  else:\n",
    "      p = 0\n",
    "  # compute R\n",
    "  if GOLD_COUNT != 0:\n",
    "      r = (TRUE_COUNT / GOLD_COUNT)\n",
    "  else:\n",
    "      r = 0\n",
    "  # compute F1\n",
    "  if (r + p) != 0:\n",
    "      f1 = ((2 * r * p) / (r + p))\n",
    "  else:\n",
    "      f1 = 0\n",
    "\n",
    "  p = round(p * 100, 2)\n",
    "  r = round(r * 100, 2)\n",
    "  f1 = round(f1 * 100, 2)\n",
    "\n",
    "  return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_f1(y_pred, y_true):\n",
    "    # flatten and convert to numpy array\n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    mask = np.where(y_true != 0)\n",
    "\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "model = BertNerModel(num_labels=6)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(),lr=5e-5,weight_decay=1e-2)\n",
    "loss_fn = CrossEntropyLoss(reduction='none', ignore_index=0)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "epochs = 5\n",
    "best_f1 = 0.0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_value = 0.0\n",
    "    model.train()\n",
    "    label_true, label_pred = [], []\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        input_ids, attention_masks, _, features, tags = batch\n",
    "        pred_tags = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "\n",
    "        loss = loss_fn(pred_tags.permute(0,2,1),tags.to(device))\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        pred_tags = F.softmax(pred_tags,dim=-1)\n",
    "        pred_tags = torch.argmax(pred_tags,dim=-1)\n",
    "\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "    \n",
    "        loss_value += loss.item()\n",
    "\n",
    "    label_train_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "\n",
    "    model.eval()\n",
    "    kw_true, kw_pred = [], []\n",
    "    label_true, label_pred = [],[]\n",
    "    for i,batch in enumerate(test_dataloader):\n",
    "      input_ids, attention_masks, tokens, features, tags = batch\n",
    "      with torch.no_grad():\n",
    "          for module in model.modules():\n",
    "              if isinstance(module, nn.Dropout):\n",
    "                  module.p = 0\n",
    "                  module.train(False)\n",
    "          pred_tags = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "          pred_tags = F.softmax(pred_tags,dim=-1)\n",
    "          pred_tags = torch.argmax(pred_tags,dim=-1)\n",
    "\n",
    "      y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "      label_true.extend(y_true)\n",
    "      label_pred.extend(y_pred)\n",
    "\n",
    "      # more balance evaluate\n",
    "      poss = []\n",
    "      for i in range(len(tags)):\n",
    "          pos = []\n",
    "          for j in range(len(tags[i])):\n",
    "              if tags[i][j] == 0:\n",
    "                  pos.append(j)\n",
    "          poss.append(pos)\n",
    "           \n",
    "      kw_true.extend(TagConvert(tags,tokens))\n",
    "      kw_pred.extend(TagConvert(pred_tags,tokens,poss))\n",
    "\n",
    "    label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "    P, R, F1 = evaluate(kw_true, kw_pred)\n",
    "    \n",
    "    if F1 > best_f1:\n",
    "        best_f1 = F1\n",
    "        torch.save(model.state_dict(),'./pretrain_pt/bert.pt')\n",
    "        \n",
    "    print(\"epoch{}:  loss:{:.2f}   train_f1_value:{:.2f}  test_f1_value:{:.2f}  kw_f1_value:{:.2f}\".format(\n",
    "        epoch+1, loss_value / len(train_dataloader), label_train_f1, label_f1, F1\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertNerModel(num_labels=6)\n",
    "model.load_state_dict(torch.load('./pretrain_pt/bert.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "kw_true, kw_pred = [], []\n",
    "label_true, label_pred = [],[]\n",
    "for i,batch in enumerate(test_dataloader):\n",
    "    input_ids, attention_masks, tokens, tags = batch\n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = 0\n",
    "                module.train(False)\n",
    "        pred_tags = model(input_ids.to(device), attention_masks.to(device))\n",
    "        pred_tags = F.softmax(pred_tags,dim=-1)\n",
    "        pred_tags = torch.argmax(pred_tags,dim=-1)\n",
    "\n",
    "    y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "    label_true.extend(y_true)\n",
    "    label_pred.extend(y_pred)\n",
    "\n",
    "    # more balance evaluate\n",
    "    poss = []\n",
    "    for i in range(len(tags)):\n",
    "        pos = []\n",
    "        for j in range(len(tags[i])):\n",
    "            if tags[i][j] == 0:\n",
    "                pos.append(j)\n",
    "        poss.append(pos)\n",
    "        \n",
    "    kw_true.extend(TagConvert(tags,tokens))\n",
    "    kw_pred.extend(TagConvert(pred_tags,tokens,poss))\n",
    "\n",
    "label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "P, R, F1 = evaluate(kw_true, kw_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P)\n",
    "print(R)\n",
    "print(F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
